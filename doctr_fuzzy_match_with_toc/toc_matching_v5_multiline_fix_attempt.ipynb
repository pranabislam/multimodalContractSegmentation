{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiline match problem:\n",
    "- horizontal information mapping doesnt translate with window style match for exact match when we use 2 lines \n",
    "- solution:\n",
    "    - choose line that has more characters and try to do some xcordinate matching with this line\n",
    "        - might need to look at character level windows? not sure\n",
    "    - extend this x cor to the ycor of line above / below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../doctr/\")\n",
    "# from doctr.models import ocr_predictor\n",
    "# from doctr.io import DocumentFile\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_dict(json_output):\n",
    "    '''\n",
    "    Given json output from OCR, construct a dictionary to better represent the data\n",
    "    '''\n",
    "    file_as_dict = {'words':[], 'ymin_ymax':[], 'page_id':[], 'page_dimensions':[], 'line_merge_next':[], 'bboxes': []}\n",
    "    for page in json_output['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for idx, line in enumerate(block['lines']):\n",
    "                ((xmin, ymin),(xmax, ymax)) = line['geometry']\n",
    "\n",
    "                \n",
    "                file_as_dict['words'].append([line['words'][i]['value'] for i in range(len(line['words']))])\n",
    "                file_as_dict['bboxes'].append([line['words'][ii]['geometry'] for ii in range(len(line['words']))])\n",
    "                \n",
    "                file_as_dict['ymin_ymax'].append((ymin, ymax))\n",
    "                file_as_dict['page_id'].append(page['page_idx'])\n",
    "                file_as_dict['page_dimensions'].append(page['dimensions'])\n",
    "\n",
    "    return file_as_dict\n",
    "\n",
    "def getIOU(segment1, segment2, threshold):\n",
    "    '''\n",
    "    Merging algorithm for line geometries. If lines are in different blocks but have similar line coordinates, we will be \n",
    "    able to \"merge\" them as one line this way. Function needed because OCR model does not always treat the same line\n",
    "    as an item within the same block\n",
    "    '''\n",
    "    ymin1, ymax1 = segment1\n",
    "    ymin2, ymax2 = segment2\n",
    "    \n",
    "    less_ymax = min(ymax1, ymax2)\n",
    "    less_ymin = min(ymin1, ymin2)\n",
    "    greater_ymax = max(ymax1, ymax2)\n",
    "    greater_ymin = max(ymin1, ymin2)\n",
    "    \n",
    "    intersection = less_ymax - greater_ymin\n",
    "    \n",
    "    # no overlap\n",
    "    if intersection < 0:\n",
    "        return False\n",
    "    \n",
    "    union = greater_ymax - less_ymin\n",
    "    \n",
    "    if intersection / union > threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_lines_to_merge(file_as_dict, threshold):\n",
    "    lines_to_merge_indices = []\n",
    "    for i in range(0, len(file_as_dict['words'])):\n",
    "        for j in range(i+1, len(file_as_dict['words'])):\n",
    "            if (file_as_dict['page_id'][i] == file_as_dict['page_id'][j]) and getIOU(file_as_dict['ymin_ymax'][i], \n",
    "                                                                                     file_as_dict['ymin_ymax'][j], \n",
    "                                                                                     threshold):\n",
    "                lines_to_merge_indices.append((i,j))\n",
    "    return lines_to_merge_indices\n",
    "\n",
    "def line_merging(lines_to_merge_indices):\n",
    "    follows = dict()\n",
    "    lines = dict()\n",
    "    for i, j in lines_to_merge_indices:\n",
    "\n",
    "        if i not in lines:\n",
    "            if i not in follows:\n",
    "                lines[i] = [i, j]\n",
    "                follows[j] = [i]\n",
    "            else:  \n",
    "                one_link_back = follows[i][0]\n",
    "                while one_link_back in follows:\n",
    "                    one_link_back = follows[one_link_back][0]\n",
    "                if j not in lines[one_link_back]:\n",
    "                    lines[follows[i][0]].append(j)\n",
    "        else:\n",
    "            lines[i].append(j)\n",
    "\n",
    "        if j not in follows:\n",
    "            follows[j] = [i]\n",
    "        else:\n",
    "            follows[j].append(i)\n",
    "    return lines, follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_file_line_by_line(file_as_dict, threshold):\n",
    "    final_file_as_dict = {'full_line':[], 'page_id':[], 'ymax_max':[], 'ymin_min':[], 'bboxes': []}\n",
    "    lines_to_merge = get_lines_to_merge(file_as_dict, threshold)\n",
    "    line_merge_map, follow_merge_map = line_merging(lines_to_merge)\n",
    "\n",
    "    # final_file_as_dict['bboxes'] = [bbox for bbox in file_as_dict['bboxes']]\n",
    "    for i in range(len(file_as_dict['words'])):\n",
    "        if i not in line_merge_map and i not in follow_merge_map:\n",
    "            final_file_as_dict['full_line'].extend([file_as_dict['words'][i]])\n",
    "            final_file_as_dict['bboxes'].extend([file_as_dict['bboxes'][i]])\n",
    "            final_file_as_dict['page_id'].append(file_as_dict['page_id'][i])\n",
    "            final_file_as_dict['ymin_min'].append(file_as_dict['ymin_ymax'][i][0])\n",
    "            final_file_as_dict['ymax_max'].append(file_as_dict['ymin_ymax'][i][1])\n",
    "            \n",
    "        else:\n",
    "            if i in line_merge_map:\n",
    "                line = []\n",
    "                ymin_min = []\n",
    "                ymax_max = []\n",
    "                bboxes = []\n",
    "                for j in line_merge_map[i]:\n",
    "                    line.extend(file_as_dict['words'][j])\n",
    "                    ymin_min.append(file_as_dict['ymin_ymax'][j][0])\n",
    "                    ymax_max.append(file_as_dict['ymin_ymax'][j][1])\n",
    "                    bboxes.extend(file_as_dict['bboxes'][j])\n",
    "                \n",
    "                final_file_as_dict['full_line'].append(line)\n",
    "                final_file_as_dict['bboxes'].append(bboxes)\n",
    "                final_file_as_dict['ymin_min'].append(ymin_min)\n",
    "                final_file_as_dict['ymax_max'].append(ymax_max)\n",
    "                final_file_as_dict['page_id'].append(file_as_dict['page_id'][i])\n",
    "                \n",
    "                #final_file_as_dict['full_line'].append([file_as_dict['words'][j] for j in line_merge_map[i]])\n",
    "                #final_file_as_dict['ymin_min'].append([file_as_dict['ymin_ymax'][j][0] for j in line_merge_map[i]])\n",
    "                #final_file_as_dict['ymax_max'].append([file_as_dict['ymin_ymax'][j][1] for j in line_merge_map[i]])\n",
    "    \n",
    "    \n",
    "    for idx, (min_element, max_element) in enumerate(zip(final_file_as_dict['ymin_min'], final_file_as_dict['ymax_max'])):\n",
    "        if type(min_element) == type(list()):\n",
    "            new_min_element = min(min_element)\n",
    "            new_max_element = max(max_element)\n",
    "            final_file_as_dict['ymin_min'][idx] = new_min_element\n",
    "            final_file_as_dict['ymax_max'][idx] = new_max_element\n",
    "    \n",
    "    print(len(final_file_as_dict['full_line']), len(final_file_as_dict['bboxes']))\n",
    "    return final_file_as_dict\n",
    "\n",
    "def get_toc_page(preprocessed_output):\n",
    "    regex_exp = r\"(table of contents|tableof(?:contents)?|(?:table\\s)?of*conten|contents?)\"\n",
    "    \n",
    "    for page_id, line in zip(preprocessed_output['page_id'],\n",
    "                             preprocessed_output['full_line']):\n",
    "        if re.search(regex_exp, \" \".join(line).lower()):\n",
    "            return page_id\n",
    "    return None\n",
    "\n",
    "def match_line(section_info, preprocessed_output, idx1, idx2,\n",
    "               subset_match_threshold, line_len_match_threshold,\n",
    "               beg_line_match_threshold, first_line_match_threshold, toc_page):\n",
    "    '''\n",
    "    replace bulk of find_start logic with this. this function operates 1 line at a time or 2 lines at a time\n",
    "    if both idx are passed in. -- need to figure out how to incorporate proper bboxes logic though\n",
    "    '''\n",
    "    \n",
    "    if preprocessed_output['page_id'][idx1] == toc_page:\n",
    "        return None, None\n",
    "    \n",
    "    if (idx1 is not None) and (idx2 is not None):\n",
    "        if preprocessed_output['page_id'][idx2] == toc_page:\n",
    "            return None, None\n",
    "        if preprocessed_output['page_id'][idx1] != preprocessed_output['page_id'][idx2]:\n",
    "            return None, None\n",
    "        \n",
    "        ## 2 line merge\n",
    "        multi_line = \" \".join(preprocessed_output['full_line'][idx1] + preprocessed_output['full_line'][idx2])\n",
    "        line1 = \" \".join(preprocessed_output['full_line'][idx1])\n",
    "        \n",
    "        beg_line = multi_line[0: len(section_info[0])*2] # not really necessary for this case but keeping it for consistency\n",
    "        \n",
    "        if (fuzz.partial_ratio(section_info[0].lower(), multi_line.lower()) > subset_match_threshold       and\n",
    "            len(multi_line) >= len(section_info[0]) * line_len_match_threshold                             and\n",
    "            fuzz.partial_ratio(section_info[0].lower(), beg_line.lower()) > beg_line_match_threshold and\n",
    "            fuzz.partial_ratio(section_info[0].lower(), line1.lower()) > first_line_match_threshold):\n",
    "\n",
    "            # check if the beginning of the section info (first 2 words- design choice )has a high match with \n",
    "            # the beginning of the matched line. \n",
    "            section_info_beg = section_info[0].split()[:2]\n",
    "            section_info_beg = \" \".join(section_info_beg)\n",
    "\n",
    "            matched_line_beg = multi_line.split()[:2]\n",
    "            matched_line_beg = \" \".join(matched_line_beg)\n",
    "\n",
    "            # if debug:\n",
    "                # print(f\"section_info_beg: {section_info_beg}\\nmatched_line_beg: {matched_line_beg}, match: {fuzz.partial_ratio(section_info_beg.lower(), matched_line_beg.lower())}\")\n",
    "\n",
    "            if fuzz.partial_ratio(section_info_beg.lower(), matched_line_beg.lower()) > subset_match_threshold:\n",
    "                ymin = preprocessed_output['ymin_min'][idx1] # min of first line\n",
    "                ymax = preprocessed_output['ymax_max'][idx2] # max of second line\n",
    "                \n",
    "                page_id = preprocessed_output['page_id'][idx1]\n",
    "                bboxes = [preprocessed_output['bboxes'][idx1], preprocessed_output['bboxes'][idx2]]\n",
    "                line2 = \" \".join(preprocessed_output['full_line'][idx2])\n",
    "                \n",
    "                return (line1, line2, section_info[0], ymin, ymax, page_id, bboxes), idx2 + 1\n",
    "            \n",
    "            # if the beginning of the section info and the matched line don't have a high match then\n",
    "            # return None\n",
    "            else:\n",
    "                return None, None\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    line = \" \".join(preprocessed_output['full_line'][idx1])\n",
    "    beg_line = line[0: len(section_info[0])*2]\n",
    "\n",
    "    if (fuzz.partial_ratio(section_info[0].lower(), line.lower()) > subset_match_threshold and\n",
    "        len(line) >= len(section_info[0]) * line_len_match_threshold                       and\n",
    "        fuzz.partial_ratio(section_info[0].lower(), beg_line.lower()) > beg_line_match_threshold):\n",
    "        \n",
    "        section_info_beg = section_info[0].split()[:2]\n",
    "        section_info_beg = \" \".join(section_info_beg)\n",
    "\n",
    "        matched_line_beg = line.split()[:2]\n",
    "        matched_line_beg = \" \".join(matched_line_beg)\n",
    "\n",
    "        # if debug:\n",
    "        # print(f\"section_info_beg: {section_info_beg}\\nmatched_line_beg: {matched_line_beg}, match: {fuzz.partial_ratio(section_info_beg.lower(), matched_line_beg.lower())}\")\n",
    "\n",
    "        if fuzz.partial_ratio(section_info_beg.lower(), matched_line_beg.lower()) > subset_match_threshold:\n",
    "            ymin = preprocessed_output['ymin_min'][idx1]\n",
    "            ymax = preprocessed_output['ymax_max'][idx1]\n",
    "            page_id = preprocessed_output['page_id'][idx1]\n",
    "            bboxes = preprocessed_output['bboxes'][idx1]\n",
    "\n",
    "            return (line, None, section_info[0], ymin, ymax, page_id, bboxes), idx1 + 1\n",
    "        \n",
    "        else:\n",
    "            None, None\n",
    "    \n",
    "    return None, None\n",
    "    \n",
    "def find_start_new(section_info, preprocessed_output,\n",
    "                   subset_match_threshold, line_len_match_threshold,\n",
    "                   beg_line_match_threshold, first_line_match_threshold, \n",
    "                   last_line_pointer):\n",
    "    '''\n",
    "    Given a toc section title, iterate all the lines in the file from the last line associated with a section title\n",
    "    going forward\n",
    "    '''\n",
    "    \n",
    "    if last_line_pointer == len(preprocessed_output['full_line']):\n",
    "        return None, last_line_pointer\n",
    "    \n",
    "    toc_page = get_toc_page(preprocessed_output)\n",
    "    \n",
    "    itertuple = zip(range(last_line_pointer, len(preprocessed_output['full_line'])),\n",
    "                    range(last_line_pointer + 1, len(preprocessed_output['full_line'])))\n",
    "    \n",
    "    for idx1, idx2 in itertuple:\n",
    "        \n",
    "        ## first try matching with first line\n",
    "        match = match_line(section_info, preprocessed_output, idx1, None,\n",
    "                           subset_match_threshold, line_len_match_threshold,\n",
    "                           beg_line_match_threshold, first_line_match_threshold, \n",
    "                           toc_page)\n",
    "        if match[0]:\n",
    "            return match\n",
    "        \n",
    "        ## let's try matching with second line only. This code is extremely inefficient but keeping for now.\n",
    "        match = match_line(section_info, preprocessed_output, idx2, None,\n",
    "                           subset_match_threshold, line_len_match_threshold,\n",
    "                           beg_line_match_threshold, first_line_match_threshold, \n",
    "                           toc_page)\n",
    "        if match[0]:\n",
    "            return match\n",
    "    \n",
    "\n",
    "        # now try matching with 2 lines \n",
    "        match = match_line(section_info, preprocessed_output, idx1, idx2,\n",
    "                           subset_match_threshold, line_len_match_threshold,\n",
    "                           beg_line_match_threshold, first_line_match_threshold, \n",
    "                           toc_page)\n",
    "        if match[0]:\n",
    "            return match\n",
    "        ## if no match, move onto the next pair of lines\n",
    "    \n",
    "    return None, last_line_pointer\n",
    "\n",
    "def get_starts_all(section_dict, preprocessed_output):\n",
    "    last_line_pointer = 0\n",
    "    #lines = list(zip(preprocessed_output2['page_id'],\n",
    "    #                  preprocessed_output2['full_line'],\n",
    "    #                  preprocessed_output2['ymin_min'],\n",
    "    #                  preprocessed_output2['ymax_max']))\n",
    "\n",
    "\n",
    "    subset_match_threshold = 80    # Design decision to only allow subset match ratios of > 80/100\n",
    "    line_len_match_threshold = 0.8 # Design decision to potentially only match document lines\n",
    "                                   # that are not much smaller than table of content label\n",
    "    beg_line_match_threshold = 80\n",
    "    first_line_match_threshold = 20 # in case of merged lines lets make sure top line is at least mildly relevant\n",
    "    starts = []\n",
    "\n",
    "    for key, section_info in section_dict.items():\n",
    "\n",
    "        start, last_line_pointer = find_start_new(section_info, preprocessed_output,\n",
    "                                                  subset_match_threshold, line_len_match_threshold,\n",
    "                                                  beg_line_match_threshold, first_line_match_threshold, \n",
    "                                                  last_line_pointer)\n",
    "        if start:\n",
    "            starts.append(start)\n",
    "        else:\n",
    "            print(f\"Couldn't match {section_info[0]} with a line. Moving onto next TOC section\")\n",
    "    \n",
    "    return starts\n",
    "\n",
    "def flatten_contract_dict(nested_dict):\n",
    "    i = 1\n",
    "    section_dict_flattened = {}\n",
    "\n",
    "    for item in nested_dict.items():\n",
    "        section_dict_flattened[i] = (item[1][0], {})\n",
    "        i += 1\n",
    "        for sub_item in item[1][1].items():\n",
    "            section_dict_flattened[i] = (sub_item[1][0], {})\n",
    "            i += 1\n",
    "    return section_dict_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bboxes(line1_words, line2_words, query, bboxes, window_size, match_score):\n",
    "    if isinstance(line1_words, str):\n",
    "        line1_words = line1_words.split()\n",
    "\n",
    "    if isinstance(line2_words, str):\n",
    "        line2_words = line2_words.split()\n",
    "\n",
    "    if line2_words:\n",
    "        full_line_words = line1_words + line2_words\n",
    "    else:\n",
    "        full_line_words = line1_words\n",
    "    \n",
    "    # print(line1_words)\n",
    "    # print(line2_words)\n",
    "    # print(full_line_words)\n",
    "    # print(bboxes)\n",
    "    \n",
    "    max_window_score, match_idx = 0, 0\n",
    "    for idx, start_idx in enumerate(range(0, len(full_line_words), window_size)):\n",
    "        window_text = \" \".join(full_line_words[start_idx : start_idx + window_size])\n",
    "        \n",
    "        window_score = process.extractBests(window_text, [query], scorer=fuzz.token_set_ratio)[0][-1]\n",
    "\n",
    "        if line2_words:\n",
    "            candidate_bboxes = bboxes[0] + bboxes[1]\n",
    "        else:\n",
    "            candidate_bboxes = bboxes[start_idx : start_idx + window_size]\n",
    "\n",
    "        if window_score >= match_score:\n",
    "            # x min is the x_left of the first bbox \n",
    "            # y_min is the min of the top left y's for each box\n",
    "            # x_min = candidate_bboxes[0][0][0]\n",
    "            x_min = min([x[0][0] for x in candidate_bboxes])\n",
    "            y_min = min([y[0][-1] for y in candidate_bboxes])\n",
    "\n",
    "            # x max is the x_right of the last bbox \n",
    "            # y_max is the max of the bottom_right y's for each box\n",
    "            # x_max = candidate_bboxes[-1][1][0]\n",
    "            x_max = max([x[1][0] for x in candidate_bboxes])\n",
    "            y_max = max([y[1][-1] for y in candidate_bboxes])\n",
    "\n",
    "            merged_bbox = [ [x_min, y_min], [x_max, y_max] ]\n",
    "            \n",
    "            return merged_bbox, window_text\n",
    "        \n",
    "        else:\n",
    "            if window_score > max_window_score:\n",
    "                max_window_score = window_score\n",
    "                match_text = window_text\n",
    "                match_candidate_bboxes = candidate_bboxes\n",
    "\n",
    "            continue\n",
    "    \n",
    "    # in case the window_text has a lower match score than with the entire string, match with the \n",
    "    # segment with highest matching score\n",
    "    # x_min = match_candidate_bboxes[0][0][0]\n",
    "    x_min = min([x[0][0] for x in match_candidate_bboxes])\n",
    "    y_min = min([y[0][-1] for y in match_candidate_bboxes])\n",
    "\n",
    "    # x max is the x_right of the last bbox \n",
    "    # y_max is the max of the bottom_right y's for each box\n",
    "    # x_max = match_candidate_bboxes[-1][1][0]\n",
    "    x_max = max([x[1][0] for x in match_candidate_bboxes])\n",
    "    y_max = max([y[1][-1] for y in match_candidate_bboxes])\n",
    "\n",
    "    merged_bbox = [ [x_min, y_min], [x_max, y_max] ]\n",
    "\n",
    "    return merged_bbox, match_text\n",
    "\n",
    "def extract_exact_match(row):\n",
    "    \n",
    "    query = row['Section Title via HTML']\n",
    "    if query is not None:\n",
    "        line1 = row.loc['Line via OCR']\n",
    "        line2 = row.loc['Line2 via OCR']\n",
    "\n",
    "        if line2 is not None:\n",
    "            full_line = line1 + line2\n",
    "        else:\n",
    "            full_line = line1\n",
    "        \n",
    "        match = process.extractBests(full_line, [query], scorer=fuzz.token_set_ratio)\n",
    "        \n",
    "        match_text, match_score = match[0]\n",
    "        window_size = len(match_text.split())\n",
    "        \n",
    "        \n",
    "        merged_bbox, exact_match_text = merge_bboxes(line1, \n",
    "                                                     line2,\n",
    "                                                     query, \n",
    "                                                     row['bboxes'], \n",
    "                                                     window_size, \n",
    "                                                     match_score)\n",
    "        \n",
    "        return merged_bbox, exact_match_text\n",
    "    \n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TOC_Labels_Set1.pkl\", \"rb\") as f:\n",
    "    section_dicts = pickle.load(f)\n",
    "section_dicts = [flatten_contract_dict(section_dict) for section_dict in section_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = {\n",
    "    'pdf_0': \n",
    "            {\n",
    "                \"name\": \"ZogenixInc_20190509_10-Q_EX-10.2_11663313_EX-10.2_Distributor Agreement\",\n",
    "                \"path\": \"pdf_0_from_list_in_discord.json\",\n",
    "                \"pdf_path\": \"full_contract_pdf/Part_I/Distributor/ZogenixInc_20190509_10-Q_EX-10.2_11663313_EX-10.2_Distributor Agreement.pdf\"\n",
    "            },\n",
    "    'pdf_1': \n",
    "            {\n",
    "                \"name\": \"PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement\",\n",
    "                \"path\": \"pdf_1_from_list_in_discord.json\",\n",
    "                \"pdf_path\": \"full_contract_pdf/Part_I/Endorsement/PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement.pdf\"\n",
    "            },\n",
    "    'pdf_2': \n",
    "            {\n",
    "                \"name\": \"OTISWORLDWIDECORP_04_03_2020-EX-10.4-INTELLECTUAL PROPERTY AGREEMENT by and among UNITED TECHNOLOGIES CORPORATION, OTIS WORLDWIDE CORPORATION and CARRIER ~1\",\n",
    "                \"path\": \"pdf_2_from_list_in_discord.json\",\n",
    "                \"pdf_path\": \"full_contract_pdf/Part_III/IP/OTISWORLDWIDECORP_04_03_2020-EX-10.4-INTELLECTUAL PROPERTY AGREEMENT by and among UNITED TECHNOLOGIES CORPORATION, OTIS WORLDWIDE CORPORATION and CARRIER ~1.PDF\"\n",
    "            },\n",
    "    'pdf_3': \n",
    "            {\n",
    "                \"name\": \"NUVEEN - REMARKETING AGREEMENT\",\n",
    "                \"path\": \"pdf_3_from_list_in_discord.json\",\n",
    "                \"pdf_path\": \"full_contract_pdf/Part_III/Marketing/NUVEEN - REMARKETING AGREEMENT.PDF\"\n",
    "            },\n",
    "    'pdf_4': \n",
    "            {\n",
    "                \"name\": \"ParatekPharmaceuticalsInc_20170505_10-KA_EX-10.29_10323872_EX-10.29_Outsourcing Agreement\",\n",
    "                \"path\": \"pdf_4_from_list_in_discord.json\",\n",
    "                \"pdf_path\": \"full_contract_pdf/Part_I/Outsourcing/ParatekPharmaceuticalsInc_20170505_10-KA_EX-10.29_10323872_EX-10.29_Outsourcing Agreement.pdf\"\n",
    "            },\n",
    " \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output using non multiline match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'pdf_2'\n",
    "json_path = json_files[file_key]['path']\n",
    "filename = json_files[file_key]['name']\n",
    "pickle_key = int(file_key.split('_')[1])\n",
    "section_dict = section_dicts[pickle_key]\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')\n",
    "\n",
    "df = pd.DataFrame(get_starts_all(section_dict, preprocessed_output)).rename(columns={0:'Line via OCR',\n",
    "                                     1:'Section Title via HTML',\n",
    "                                     2:'ymin',\n",
    "                                     3:'ymax',\n",
    "                                     4:'page_id',\n",
    "                                     5:'bboxes'})\n",
    "# df['exact_match_bbox'], df['exact_match_text'] = zip(*df.apply(lambda row: extract_exact_match(row), axis=1))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output using multiline match (find_start_new fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'pdf_2'\n",
    "json_path = json_files[file_key]['path']\n",
    "filename = json_files[file_key]['name']\n",
    "pickle_key = int(file_key.split('_')[1])\n",
    "section_dict = section_dicts[pickle_key]\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')\n",
    "\n",
    "df = pd.DataFrame(get_starts_all(section_dict, preprocessed_output)).rename(columns={0:'Line via OCR',\n",
    "                                                                                     1:'Line2 via OCR',\n",
    "                                     2:'Section Title via HTML',\n",
    "                                     3:'ymin',\n",
    "                                     4:'ymax',\n",
    "                                     5:'page_id',\n",
    "                                     6:'bboxes'})\n",
    "\n",
    "df['exact_match_bbox'], df['exact_match_text'] = zip(*df.apply(lambda row: extract_exact_match(row), axis=1))                                     \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_dict[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It looks like its working but theres this edge case now where the second line is a good match and the first line is irrelevant but is carried in. Let's add a first_line_match_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'pdf_1'\n",
    "json_path = json_files[file_key]['path']\n",
    "filename = json_files[file_key]['name']\n",
    "pickle_key = int(file_key.split('_')[1])\n",
    "section_dict = section_dicts[pickle_key]\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')\n",
    "\n",
    "df = pd.DataFrame(get_starts_all(section_dict, preprocessed_output)).rename(columns={0:'Line via OCR',\n",
    "                                                                                     1:'Line2 via OCR',\n",
    "                                     2:'Section Title via HTML',\n",
    "                                     3:'ymin',\n",
    "                                     4:'ymax',\n",
    "                                     5:'page_id',\n",
    "                                     6:'bboxes'})\n",
    "df['exact_match_bbox'], df['exact_match_text'] = zip(*df.apply(lambda row: extract_exact_match(row), axis=1))   \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuzz.partial_ratio('4.6 Drug Safety and Pharmacovigilance System including Global Safety Database'.lower(), \n",
    "                   'Information relevant to such] pharmacovigilance activities.'.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay we will suffer from this false merged line positive case. This occurs three times in this document. See dataframe line 4, 7, 19\n",
    "\n",
    "## Let's try a simple inefficient fix where we check both lines separately first then try merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'pdf_1'\n",
    "json_path = json_files[file_key]['path']\n",
    "filename = json_files[file_key]['name']\n",
    "pickle_key = int(file_key.split('_')[1])\n",
    "section_dict = section_dicts[pickle_key]\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')\n",
    "\n",
    "df = pd.DataFrame(get_starts_all(section_dict, preprocessed_output)).rename(columns={0:'Line via OCR',\n",
    "                                                                                     1:'Line2 via OCR',\n",
    "                                     2:'Section Title via HTML',\n",
    "                                     3:'ymin',\n",
    "                                     4:'ymax',\n",
    "                                     5:'page_id',\n",
    "                                     6:'bboxes'})\n",
    "df['exact_match_bbox'], df['exact_match_text'] = zip(*df.apply(lambda row: extract_exact_match(row), axis=1))   \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import math\n",
    "from pdf2image import convert_from_path, convert_from_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_images(poppler_path, parent_path, pdf_path):\n",
    "    pdf_name = pdf_path.split(\"/\")[-1][:-4]\n",
    "    print(f\"Converting pdf to image for {pdf_name}\")\n",
    "    pdf_name = pdf_name if len(pdf_name) < 20 else pdf_name[:20]\n",
    "    full_output_folder = parent_path + pdf_name\n",
    "    if not os.path.exists(full_output_folder):\n",
    "        os.mkdir(full_output_folder)\n",
    "        print(f\"Saving images in {full_output_folder}\")\n",
    "    convert_from_path(pdf_path=pdf_path, output_folder=full_output_folder, poppler_path=poppler_path, fmt='jpeg')\n",
    "    return full_output_folder\n",
    "\n",
    "def write_bbox_images(pdf_parsed_df, full_output_folder):\n",
    "    for page_id in pdf_parsed_df['page_id'].unique():\n",
    "        sub_df = df[df['page_id'] == page_id].copy()\n",
    "        \n",
    "        img_page_id = f\"{page_id + 1}.jpg\"\n",
    "        img_name = list(filter(lambda x: True if img_page_id in x else False, os.listdir(full_output_folder)))[0]\n",
    "        img_read_path = full_output_folder + \"/\" + img_name\n",
    "        img_write_parent_path = f\"{full_output_folder}/bboxes/\"\n",
    "        \n",
    "        if not os.path.exists(img_write_parent_path):\n",
    "            os.mkdir(img_write_parent_path)\n",
    "        \n",
    "        img_write_full_path = img_write_parent_path + img_name\n",
    "        img = cv2.imread(img_read_path)\n",
    "        width, height = PIL.Image.open(img_read_path).size\n",
    "        \n",
    "        for idx, row in sub_df.iterrows():\n",
    "            (x1, y1), (x2, y2) = row['exact_match_bbox']\n",
    "            \n",
    "            x1 = math.floor(width * x1)\n",
    "            x2 = math.ceil(width * x2)\n",
    "            y1 = math.floor(height * y1)\n",
    "            y2 = math.ceil(height * y2)\n",
    "            \n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 2)\n",
    "            print(f\"Drew bounding boxes for {img_name} page \")\n",
    "        cv2.imwrite(img_write_full_path, img)\n",
    "        print(f\"Wrote image with bboxes @ {img_write_full_path}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'pdf_1'\n",
    "output_path = \"../../data/pipeline_outputs/cuad_image_samples/\"\n",
    "pdf_dir = \"../../../CUAD_v1/\"\n",
    "pdf_path = os.path.join(os.path.abspath(pdf_dir), json_files[file_key][\"pdf_path\"])\n",
    "print(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = os.path.basename(pdf_path).rsplit(\".\", 1)[0].split('_')[0]\n",
    "print(pdf_name)\n",
    "full_output_folder = os.path.join(os.path.abspath(output_path), pdf_name)\n",
    "print(full_output_folder)\n",
    "if not os.path.exists(full_output_folder):\n",
    "    os.makedirs(full_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_from_path(pdf_path=pdf_path, output_folder=full_output_folder, poppler_path=poppler_path, fmt='jpeg')\n",
    "convert_from_path(pdf_path=pdf_path, output_folder=full_output_folder, fmt='jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_bbox_images(pdf_parsed_df=df, full_output_folder=full_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce6c9754676e356b60dc94fe2b04dd2fb66b8461b319750f347e70624e33de59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
