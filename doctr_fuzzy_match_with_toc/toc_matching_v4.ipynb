{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.path.append(\"../doctr/\")\n",
    "# from doctr.models import ocr_predictor\n",
    "# from doctr.io import DocumentFile\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_dict(json_output):\n",
    "    '''\n",
    "    Given json output from OCR, construct a dictionary to better represent the data\n",
    "    '''\n",
    "    file_as_dict = {'words':[], 'ymin_ymax':[], 'page_id':[], 'page_dimensions':[], 'line_merge_next':[], 'bboxes': []}\n",
    "    for page in json_output['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for idx, line in enumerate(block['lines']):\n",
    "                ((xmin, ymin),(xmax, ymax)) = line['geometry']\n",
    "\n",
    "                \n",
    "                file_as_dict['words'].append([line['words'][i]['value'] for i in range(len(line['words']))])\n",
    "                file_as_dict['bboxes'].append([line['words'][ii]['geometry'] for ii in range(len(line['words']))])\n",
    "                \n",
    "                file_as_dict['ymin_ymax'].append((ymin, ymax))\n",
    "                file_as_dict['page_id'].append(page['page_idx'])\n",
    "                file_as_dict['page_dimensions'].append(page['dimensions'])\n",
    "\n",
    "    return file_as_dict\n",
    "\n",
    "def getIOU(segment1, segment2, threshold):\n",
    "    '''\n",
    "    Merging algorithm for line geometries. If lines are in different blocks but have similar line coordinates, we will be \n",
    "    able to \"merge\" them as one line this way. Function needed because OCR model does not always treat the same line\n",
    "    as an item within the same block\n",
    "    '''\n",
    "    ymin1, ymax1 = segment1\n",
    "    ymin2, ymax2 = segment2\n",
    "    \n",
    "    less_ymax = min(ymax1, ymax2)\n",
    "    less_ymin = min(ymin1, ymin2)\n",
    "    greater_ymax = max(ymax1, ymax2)\n",
    "    greater_ymin = max(ymin1, ymin2)\n",
    "    \n",
    "    intersection = less_ymax - greater_ymin\n",
    "    \n",
    "    # no overlap\n",
    "    if intersection < 0:\n",
    "        return False\n",
    "    \n",
    "    union = greater_ymax - less_ymin\n",
    "    \n",
    "    if intersection / union > threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_lines_to_merge(file_as_dict, threshold):\n",
    "    lines_to_merge_indices = []\n",
    "    for i in range(0, len(file_as_dict['words'])):\n",
    "        for j in range(i+1, len(file_as_dict['words'])):\n",
    "            if (file_as_dict['page_id'][i] == file_as_dict['page_id'][j]) and getIOU(file_as_dict['ymin_ymax'][i], \n",
    "                                                                                     file_as_dict['ymin_ymax'][j], \n",
    "                                                                                     threshold):\n",
    "                lines_to_merge_indices.append((i,j))\n",
    "    return lines_to_merge_indices\n",
    "\n",
    "def line_merging(lines_to_merge_indices):\n",
    "    follows = dict()\n",
    "    lines = dict()\n",
    "    for i, j in lines_to_merge_indices:\n",
    "\n",
    "        if i not in lines:\n",
    "            if i not in follows:\n",
    "                lines[i] = [i, j]\n",
    "                follows[j] = [i]\n",
    "            else:  \n",
    "                one_link_back = follows[i][0]\n",
    "                while one_link_back in follows:\n",
    "                    one_link_back = follows[one_link_back][0]\n",
    "                if j not in lines[one_link_back]:\n",
    "                    lines[follows[i][0]].append(j)\n",
    "        else:\n",
    "            lines[i].append(j)\n",
    "\n",
    "        if j not in follows:\n",
    "            follows[j] = [i]\n",
    "        else:\n",
    "            follows[j].append(i)\n",
    "    return lines, follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_file_line_by_line(file_as_dict, threshold):\n",
    "    final_file_as_dict = {'full_line':[], 'page_id':[], 'ymax_max':[], 'ymin_min':[], 'bboxes': []}\n",
    "    lines_to_merge = get_lines_to_merge(file_as_dict, threshold)\n",
    "    line_merge_map, follow_merge_map = line_merging(lines_to_merge)\n",
    "\n",
    "    # final_file_as_dict['bboxes'] = [bbox for bbox in file_as_dict['bboxes']]\n",
    "    for i in range(len(file_as_dict['words'])):\n",
    "        if i not in line_merge_map and i not in follow_merge_map:\n",
    "            final_file_as_dict['full_line'].extend([file_as_dict['words'][i]])\n",
    "            final_file_as_dict['bboxes'].extend([file_as_dict['bboxes'][i]])\n",
    "            final_file_as_dict['page_id'].append(file_as_dict['page_id'][i])\n",
    "            final_file_as_dict['ymin_min'].append(file_as_dict['ymin_ymax'][i][0])\n",
    "            final_file_as_dict['ymax_max'].append(file_as_dict['ymin_ymax'][i][1])\n",
    "            \n",
    "        else:\n",
    "            if i in line_merge_map:\n",
    "                line = []\n",
    "                ymin_min = []\n",
    "                ymax_max = []\n",
    "                bboxes = []\n",
    "                for j in line_merge_map[i]:\n",
    "                    line.extend(file_as_dict['words'][j])\n",
    "                    ymin_min.append(file_as_dict['ymin_ymax'][j][0])\n",
    "                    ymax_max.append(file_as_dict['ymin_ymax'][j][1])\n",
    "                    bboxes.extend(file_as_dict['bboxes'][j])\n",
    "                \n",
    "                final_file_as_dict['full_line'].append(line)\n",
    "                final_file_as_dict['bboxes'].append(bboxes)\n",
    "                final_file_as_dict['ymin_min'].append(ymin_min)\n",
    "                final_file_as_dict['ymax_max'].append(ymax_max)\n",
    "                final_file_as_dict['page_id'].append(file_as_dict['page_id'][i])\n",
    "                \n",
    "                #final_file_as_dict['full_line'].append([file_as_dict['words'][j] for j in line_merge_map[i]])\n",
    "                #final_file_as_dict['ymin_min'].append([file_as_dict['ymin_ymax'][j][0] for j in line_merge_map[i]])\n",
    "                #final_file_as_dict['ymax_max'].append([file_as_dict['ymin_ymax'][j][1] for j in line_merge_map[i]])\n",
    "    \n",
    "    \n",
    "    for idx, (min_element, max_element) in enumerate(zip(final_file_as_dict['ymin_min'], final_file_as_dict['ymax_max'])):\n",
    "        if type(min_element) == type(list()):\n",
    "            new_min_element = min(min_element)\n",
    "            new_max_element = max(max_element)\n",
    "            final_file_as_dict['ymin_min'][idx] = new_min_element\n",
    "            final_file_as_dict['ymax_max'][idx] = new_max_element\n",
    "    \n",
    "    print(len(final_file_as_dict['full_line']), len(final_file_as_dict['bboxes']))\n",
    "    return final_file_as_dict\n",
    "\n",
    "def get_toc_page(preprocessed_output):\n",
    "    regex_exp = r\"(table of contents|table*of*(?:contents)?|(?:table\\s)?of*conten|contents?)\"\n",
    "    for page_id, line in zip(preprocessed_output['page_id'],\n",
    "                             preprocessed_output['full_line']):\n",
    "        \n",
    "        ref_str = \" \".join(line).lower()\n",
    "        matches = re.search(regex_exp, ref_str)\n",
    "        if matches is not None:\n",
    "            print(f\"page_id: {page_id}\")\n",
    "            return page_id\n",
    "    return None\n",
    "\n",
    "def find_start(section_info, preprocessed_output,\n",
    "               subset_match_threshold, line_len_match_threshold,\n",
    "               beg_line_match_threshold, last_line_pointer):\n",
    "    '''\n",
    "    Given a toc section title, iterate all the lines in the file from the last line associated with a section title\n",
    "    going forward\n",
    "    '''\n",
    "    \n",
    "    if last_line_pointer == len(preprocessed_output['full_line']):\n",
    "        return None, last_line_pointer\n",
    "    \n",
    "    toc_page = get_toc_page(preprocessed_output)\n",
    "    \n",
    "    for idx in range(last_line_pointer, len(preprocessed_output['full_line'])):\n",
    "        \n",
    "        if preprocessed_output['page_id'][idx] == toc_page:\n",
    "            continue\n",
    "        line = \" \".join(preprocessed_output['full_line'][idx])\n",
    "        bboxes = preprocessed_output['bboxes'][idx]\n",
    "    \n",
    "        beg_line = line[0: len(section_info[0])*2]\n",
    "\n",
    "        if (fuzz.partial_ratio(section_info[0].lower(), line.lower()) > subset_match_threshold and\n",
    "            len(line) >= len(section_info[0]) * line_len_match_threshold                       and\n",
    "            fuzz.partial_ratio(section_info[0].lower(), beg_line.lower()) > beg_line_match_threshold):\n",
    "            \n",
    "            ymin = preprocessed_output['ymin_min'][idx]\n",
    "            ymax = preprocessed_output['ymax_max'][idx]\n",
    "            page_id = preprocessed_output['page_id'][idx]\n",
    "            \n",
    "            return (line, section_info[0], ymin, ymax, page_id, bboxes), idx + 1\n",
    "    \n",
    "    return None, last_line_pointer\n",
    "\n",
    "def get_starts_all(section_dict, preprocessed_output):\n",
    "    last_line_pointer = 0\n",
    "    #lines = list(zip(preprocessed_output2['page_id'],\n",
    "    #                  preprocessed_output2['full_line'],\n",
    "    #                  preprocessed_output2['ymin_min'],\n",
    "    #                  preprocessed_output2['ymax_max']))\n",
    "\n",
    "\n",
    "    subset_match_threshold = 80    # Design decision to only allow subset match ratios of > 80/100\n",
    "    line_len_match_threshold = 0.8 # Design decision to potentially only match document lines\n",
    "                                   # that are not much smaller than table of content label\n",
    "    beg_line_match_threshold = 80\n",
    "    starts = []\n",
    "    toc_page = get_toc_page(preprocessed_output)\n",
    "\n",
    "    for key, section_info in section_dict.items():\n",
    "\n",
    "        start, last_line_pointer = find_start(section_info, preprocessed_output,\n",
    "                                               subset_match_threshold, line_len_match_threshold,\n",
    "                                               beg_line_match_threshold, last_line_pointer)\n",
    "        if start:\n",
    "            starts.append(start)\n",
    "        else:\n",
    "            print(f\"Couldn't match {section_info[0]} with a line. Moving onto next TOC section\")\n",
    "    \n",
    "    return starts\n",
    "def flatten_contract_dict(nested_dict):\n",
    "    i = 1\n",
    "    section_dict_flattened = {}\n",
    "\n",
    "    for item in nested_dict.items():\n",
    "        section_dict_flattened[i] = (item[1][0], {})\n",
    "        i += 1\n",
    "        for sub_item in item[1][1].items():\n",
    "            section_dict_flattened[i] = (sub_item[1][0], {})\n",
    "            i += 1\n",
    "    return section_dict_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bboxes(words, query, bboxes, window_size, match_score):\n",
    "    \n",
    "    max_window_score, match_idx = 0, 0\n",
    "    for idx, start_idx in enumerate(range(0, len(words), window_size)):\n",
    "        window_text = \" \".join(words[start_idx : start_idx + window_size])\n",
    "        \n",
    "        window_score = process.extractBests(window_text, [query], scorer=fuzz.token_set_ratio)[0][-1]\n",
    "        candidate_bboxes = bboxes[start_idx : start_idx + window_size]\n",
    "\n",
    "        if window_score >= match_score:\n",
    "            # x min is the x_left of the first bbox \n",
    "            # y_min is the min of the top left y's for each box\n",
    "            x_min = candidate_bboxes[0][0][0]\n",
    "            y_min = min([y[0][-1] for y in candidate_bboxes])\n",
    "\n",
    "            # x max is the x_right of the last bbox \n",
    "            # y_max is the max of the bottom_right y's for each box\n",
    "            x_max = candidate_bboxes[-1][1][0]\n",
    "            y_max = max([y[1][-1] for y in candidate_bboxes])\n",
    "\n",
    "            merged_bbox = [ [x_min, y_min], [x_max, y_max] ]\n",
    "            \n",
    "            return merged_bbox, window_text\n",
    "        \n",
    "        else:\n",
    "            if window_score > max_window_score:\n",
    "                max_window_score = window_score\n",
    "                match_text = window_text\n",
    "                match_candidate_bboxes = candidate_bboxes\n",
    "\n",
    "            continue\n",
    "    \n",
    "    # in case the window_text has a lower match score than with the entire string, match with the \n",
    "    # segment with highest matching score\n",
    "    x_min = match_candidate_bboxes[0][0][0]\n",
    "    y_min = min([y[0][-1] for y in match_candidate_bboxes])\n",
    "\n",
    "    # x max is the x_right of the last bbox \n",
    "    # y_max is the max of the bottom_right y's for each box\n",
    "    x_max = match_candidate_bboxes[-1][1][0]\n",
    "    y_max = max([y[1][-1] for y in match_candidate_bboxes])\n",
    "\n",
    "    merged_bbox = [ [x_min, y_min], [x_max, y_max] ]\n",
    "\n",
    "    return merged_bbox, match_text\n",
    "\n",
    "def extract_exact_match(row):\n",
    "    \n",
    "    query = row['Section Title via HTML']\n",
    "    if query is not None:\n",
    "\n",
    "        match = process.extractBests(row.loc['Line via OCR'], [query], scorer=fuzz.token_set_ratio)\n",
    "        \n",
    "        match_text, match_score = match[0]\n",
    "        window_size = len(match_text.split())\n",
    "        \n",
    "        \n",
    "        merged_bbox, exact_match_text = merge_bboxes(row['Line via OCR'].split(), \n",
    "                                                     query, \n",
    "                                                     row['bboxes'], \n",
    "                                                     window_size, \n",
    "                                                     match_score)\n",
    "        \n",
    "        return merged_bbox, exact_match_text\n",
    "    \n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toc_page_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TOC_Labels_Set1.pkl\", \"rb\") as f:\n",
    "    section_dicts = pickle.load(f)\n",
    "section_dicts = [flatten_contract_dict(section_dict) for section_dict in section_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = {\n",
    "    'pdf_0': \n",
    "            {\n",
    "                \"name\": \"ZogenixInc_20190509_10-Q_EX-10.2_11663313_EX-10.2_Distributor Agreement\",\n",
    "                \"path\": \"pdf_0_from_list_in_discord.json\"\n",
    "            },\n",
    "    'pdf_1': \n",
    "            {\n",
    "                \"name\": \"PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement\",\n",
    "                \"path\": \"pdf_1_from_list_in_discord.json\"\n",
    "            },\n",
    "    'pdf_2': \n",
    "            {\n",
    "                \"name\": \"OTISWORLDWIDECORP_04_03_2020-EX-10.4-INTELLECTUAL PROPERTY AGREEMENT by and among UNITED TECHNOLOGIES CORPORATION, OTIS WORLDWIDE CORPORATION and CARRIER ~1\",\n",
    "                \"path\": \"pdf_2_from_list_in_discord.json\"\n",
    "            },\n",
    "    'pdf_3': \n",
    "            {\n",
    "                \"name\": \"NUVEEN - REMARKETING AGREEMENT\",\n",
    "                \"path\": \"pdf_3_from_list_in_discord.json\"\n",
    "            },\n",
    "    'pdf_4': \n",
    "            {\n",
    "                \"name\": \"'ParatekPharmaceuticalsInc_20170505_10-KA_EX-10.29_10323872_EX-10.29_Outsourcing Agreement\",\n",
    "                \"path\": \"pdf_4_from_list_in_discord.json\"\n",
    "            },\n",
    " \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the outputs now for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'pdf_4'\n",
    "json_path = json_files[file_key]['path']\n",
    "filename = json_files[file_key]['name']\n",
    "pickle_key = int(file_key.split('_')[1])\n",
    "section_dict = section_dicts[pickle_key]\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')\n",
    "\n",
    "df = pd.DataFrame(get_starts_all(section_dict, preprocessed_output)).rename(columns={0:'Line via OCR',\n",
    "                                     1:'Section Title via HTML',\n",
    "                                     2:'ymin',\n",
    "                                     3:'ymax',\n",
    "                                     4:'page_id',\n",
    "                                     5:'bboxes'})\n",
    "df['exact_match_bbox'], df['exact_match_text'] = zip(*df.apply(lambda row: extract_exact_match(row), axis=1))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json_output_paths = ['pdf_0_from_list_in_discord.json',\n",
    "                     'pdf_1_from_list_in_discord.json',\n",
    "                     'pdf_2_from_list_in_discord.json',\n",
    "                     'pdf_3_from_list_in_discord.json',\n",
    "                     'example_json_output.json',]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(json_output_paths[0], 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"TOC_Labels_Set1.pkl\", \"rb\") as f:\n",
    "    section_dicts = pickle.load(f)\n",
    "section_dicts = [flatten_contract_dict(section_dict) for section_dict in section_dicts]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "section_dict = section_dicts[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame(get_starts_all(section_dict, preprocessed_output)).rename(columns={0:'Line via OCR',\n",
    "                                     1:'Section Title via HTML',\n",
    "                                     2:'ymin',\n",
    "                                     3:'ymax',\n",
    "                                     4:'page_id',\n",
    "                                     5:'bboxes'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Commentary:\n",
    "For zogenix, it looks like it matched everything except the lines with \"article\" in it. This is because the TOC has article ..[SOME LABEL] in one line whereas the rest of the contract has it in 2 lines. We can make a somewhat reasonably fix for this edge case.\n",
    "\n",
    "Let's do the next contract now"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json_index = 1\n",
    "section_dict_index = 2 # json files skip the endorsement agreement since we already did that. sorry this is confusing will fix\n",
    "\n",
    "section_dict = section_dicts[section_dict_index]\n",
    "\n",
    "with open(json_output_paths[json_index], 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_starts_all(section_dict, preprocessed_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'OTISWORLDWIDECORP_04_03_2020-EX-10.4-INTELLECTUAL PROPERTY AGREEMENT by and among UNITED TECHNOLOGIES CORPORATION, OTIS WORLDWIDE CORPORATION and CARRIER ~1'\n",
    "\n",
    "Matches the TOC page. This is likely because the ocr must have mispelled table of contents. We can try to fix this via some other kind of broader match for TOC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocessed_output['full_line']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Confirmed the above hypothesis. This pdf is formatted poorly with characters having no space. This hurts OCR\n",
    "\n",
    "Let's see the next document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json_index = 2\n",
    "section_dict_index = 3 # json files skip the endorsement agreement since we already did that. sorry this is confusing will fix\n",
    "\n",
    "section_dict = section_dicts[section_dict_index]\n",
    "\n",
    "with open(json_output_paths[json_index], 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_starts_all(section_dict, preprocessed_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(section_dict), len(get_starts_all(section_dict, preprocessed_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like NUVEEN - REMARKETING AGREEMENT is a perfect match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big concern for bias in labeling data: Will many contract section headers be biased to these type of contracts where the header is normal font sized and usually in line with other words?\n",
    "\n",
    "On to the next example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json_index = 3\n",
    "section_dict_index = 4 # json files skip the endorsement agreement since we already did that. sorry this is confusing will fix\n",
    "\n",
    "section_dict = section_dicts[section_dict_index]\n",
    "\n",
    "with open(json_output_paths[json_index], 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "file_as_dict = get_file_by_dict(json_output)\n",
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.65)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_starts_all(section_dict, preprocessed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it matches the table of contents again...let's confirm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocessed_output['full_line']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh wait no it actually matched the lines! It just looked like TOC matches but it was not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of 5 articles here are results:\n",
    "1. PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement\n",
    "    - perfect match\n",
    "\n",
    "2. ZogenixInc_20190509_10-Q_EX-10.2_11663313_EX-10.2_Distributor Agreement \n",
    "    - many good matches; doesn't match lines where TOC label is actually 2 lines in the document (specifically cases where the label is \"ARTICLE X ....\"\n",
    "\n",
    "3. 'OTISWORLDWIDECORP_04_03_2020-EX-10.4-INTELLECTUAL PROPERTY AGREEMENT by and among UNITED TECHNOLOGIES CORPORATION, OTIS WORLDWIDE CORPORATION and CARRIER ~1'\n",
    "    - matched the TOC page itself. Need better method to find that page. Use state dictionary and match twice method OR fuzzy matching OR some other method\n",
    "    \n",
    "4. NUVEEN - REMARKETING AGREEMENT i\n",
    "    - Perfect match\n",
    "    \n",
    "5. ParatekPharmaceuticalsInc_20170505_10-KA_EX-10.29_10323872_EX-10.29_Outsourcing Agreement\n",
    "    - Looks like a perfect match except for these two lines in the actual TOC that are actually ambiguous. Not sure what they even mean. The two lines were:\n",
    "        - 3.8 [* * *]\n",
    "        - 3.9 [* * * ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge code 11-10-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "toc_page_id = df.loc[df['Line via OCR'].str.contains(r'table of contents|tableof|table of', case=False), 'page_id'] \n",
    "if not toc_page_id.empty:\n",
    "    toc_page_id = toc_page_id.iloc[0] \n",
    "\n",
    "toc_page_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return back to above later and integrate regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce6c9754676e356b60dc94fe2b04dd2fb66b8461b319750f347e70624e33de59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
