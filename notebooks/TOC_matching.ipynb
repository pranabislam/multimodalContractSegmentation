{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements: \n",
    "- https://github.com/mindee/doctr: I did a git clone; pip install was not working. Also install GTK (for windows; instructions in the repo). Mac should be able to install something similar.\n",
    "- Pytorch\n",
    "- pandas\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We run OCR on CUAD_v1\\full_contract_pdf\\Part_I\\Endorsement\\PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../doctr/\")\n",
    "from doctr.models import ocr_predictor\n",
    "from doctr.io import DocumentFile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r\"C:\\Users\\islam\\Desktop\\shortcutpaths\\CUAD_v1\\CUAD_v1\\full_contract_pdf\\Part_I\\Endorsement\\PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement.pdf\".replace(\"\\\\\",\"/\",)\n",
    "model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)\n",
    "\n",
    "doc = DocumentFile.from_pdf(path)\n",
    "\n",
    "result = model(doc)\n",
    "\n",
    "json_output = result.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sukrit and maybe shaan, you guys didn't install doctr so i dumped the json file so you guys can just read the OCR output that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_json_output.json', 'r', encoding='utf-8') as f:\n",
    "    json_output = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure to read the json file and store as json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we define functions to parse and merge relevant lines in the document after OCR gave us json structure. You all can ignore the functions and simply focus on the output. I stored the output of all of these functions as \"preprocessed_output\" variable. You can literally copy paste the output cell in this notebook when I run \"preprocessed_output['full_line']\" OR read in the json_output and simply run the cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_dict(json_output):\n",
    "    '''\n",
    "    Given json output from OCR, construct a dictionary to better represent the data\n",
    "    '''\n",
    "    file_as_dict = {'words':[], 'ymin_ymax':[], 'page_id':[], 'page_dimensions':[], 'line_merge_next':[], 'bboxes': []}\n",
    "    for page in json_output['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for idx, line in enumerate(block['lines']):\n",
    "                ((xmin, ymin),(xmax, ymax)) = line['geometry']\n",
    "\n",
    "                \n",
    "                file_as_dict['words'].append([line['words'][i]['value'] for i in range(len(line['words']))])\n",
    "                file_as_dict['bboxes'].append([line['words'][ii]['geometry'] for ii in range(len(line['words']))])\n",
    "                \n",
    "                file_as_dict['ymin_ymax'].append((ymin, ymax))\n",
    "                file_as_dict['page_id'].append(page['page_idx'])\n",
    "                file_as_dict['page_dimensions'].append(page['dimensions'])\n",
    "                # file_as_dict['line_merge_next'].append(None)\n",
    "    return file_as_dict\n",
    "\n",
    "def getIOU(segment1, segment2, threshold):\n",
    "    '''\n",
    "    Merging algorithm for line geometries. If lines are in different blocks but have similar line coordinates, we will be \n",
    "    able to \"merge\" them as one line this way. Function needed because OCR model does not always treat the same line\n",
    "    as an item within the same block\n",
    "    '''\n",
    "    ymin1, ymax1 = segment1\n",
    "    ymin2, ymax2 = segment2\n",
    "    \n",
    "    less_ymax = min(ymax1, ymax2)\n",
    "    less_ymin = min(ymin1, ymin2)\n",
    "    greater_ymax = max(ymax1, ymax2)\n",
    "    greater_ymin = max(ymin1, ymin2)\n",
    "    \n",
    "    intersection = less_ymax - greater_ymin\n",
    "    \n",
    "    # no overlap\n",
    "    if intersection < 0:\n",
    "        return False\n",
    "    \n",
    "    union = greater_ymax - less_ymin\n",
    "    \n",
    "    if intersection / union > threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_lines_to_merge(file_as_dict, threshold):\n",
    "    lines_to_merge_indices = []\n",
    "    for i in range(0, len(file_as_dict['words'])):\n",
    "        for j in range(i+1, len(file_as_dict['words'])):\n",
    "            if (file_as_dict['page_id'][i] == file_as_dict['page_id'][j]) and getIOU(file_as_dict['ymin_ymax'][i], \n",
    "                                                                                     file_as_dict['ymin_ymax'][j], \n",
    "                                                                                     threshold):\n",
    "                lines_to_merge_indices.append((i,j))\n",
    "    return lines_to_merge_indices\n",
    "\n",
    "def line_merging(lines_to_merge_indices):\n",
    "    follows = dict()\n",
    "    lines = dict()\n",
    "    for i, j in lines_to_merge_indices:\n",
    "\n",
    "        if i not in lines:\n",
    "            if i not in follows:\n",
    "                lines[i] = [i, j]\n",
    "            else:\n",
    "                if j not in lines[follows[i][0]]:\n",
    "                    lines[follows[i][0]].append(j)\n",
    "        else:\n",
    "            lines[i].append(j)\n",
    "\n",
    "        if j not in follows:\n",
    "            follows[j] = [i]\n",
    "        else:\n",
    "            follows[j].append(i)\n",
    "    return lines, follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get the whole file (with merged lines) as an array of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_as_dict = get_file_by_dict(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_file_line_by_line(file_as_dict, threshold):\n",
    "    final_file_as_dict = {'full_line':[], 'page_id':[], 'ymax_max':[], 'ymin_min':[], 'bboxes': []}\n",
    "    lines_to_merge = get_lines_to_merge(file_as_dict, threshold)\n",
    "    line_merge_map, follow_merge_map = line_merging(lines_to_merge)\n",
    "\n",
    "    # final_file_as_dict['bboxes'] = [bbox for bbox in file_as_dict['bboxes']]\n",
    "    for i in range(len(file_as_dict['words'])):\n",
    "        if i not in line_merge_map and i not in follow_merge_map:\n",
    "            final_file_as_dict['full_line'].extend([file_as_dict['words'][i]])\n",
    "            final_file_as_dict['bboxes'].extend([file_as_dict['bboxes'][i]])\n",
    "            final_file_as_dict['page_id'].append(file_as_dict['page_id'][i])\n",
    "            final_file_as_dict['ymin_min'].append(file_as_dict['ymin_ymax'][i][0])\n",
    "            final_file_as_dict['ymax_max'].append(file_as_dict['ymin_ymax'][i][1])\n",
    "            \n",
    "        else:\n",
    "            if i in line_merge_map:\n",
    "                line = []\n",
    "                ymin_min = []\n",
    "                ymax_max = []\n",
    "                bboxes = []\n",
    "                for j in line_merge_map[i]:\n",
    "                    line.extend(file_as_dict['words'][j])\n",
    "                    ymin_min.append(file_as_dict['ymin_ymax'][j][0])\n",
    "                    ymax_max.append(file_as_dict['ymin_ymax'][j][1])\n",
    "                    bboxes.extend(file_as_dict['bboxes'][j])\n",
    "                \n",
    "                final_file_as_dict['full_line'].append(line)\n",
    "                final_file_as_dict['bboxes'].append(bboxes)\n",
    "                final_file_as_dict['ymin_min'].append(ymin_min)\n",
    "                final_file_as_dict['ymax_max'].append(ymax_max)\n",
    "                final_file_as_dict['page_id'].append(file_as_dict['page_id'][i])\n",
    "                \n",
    "                #final_file_as_dict['full_line'].append([file_as_dict['words'][j] for j in line_merge_map[i]])\n",
    "                #final_file_as_dict['ymin_min'].append([file_as_dict['ymin_ymax'][j][0] for j in line_merge_map[i]])\n",
    "                #final_file_as_dict['ymax_max'].append([file_as_dict['ymin_ymax'][j][1] for j in line_merge_map[i]])\n",
    "    \n",
    "    \n",
    "    for idx, (min_element, max_element) in enumerate(zip(final_file_as_dict['ymin_min'], final_file_as_dict['ymax_max'])):\n",
    "        if type(min_element) == type(list()):\n",
    "            new_min_element = min(min_element)\n",
    "            new_max_element = max(max_element)\n",
    "            final_file_as_dict['ymin_min'][idx] = new_min_element\n",
    "            final_file_as_dict['ymax_max'][idx] = new_max_element\n",
    "    \n",
    "    print(len(final_file_as_dict['full_line']), len(final_file_as_dict['bboxes']))\n",
    "    return final_file_as_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the final variable that will contain the info we need where each item in preprocessed_output['full_line'] is a line in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(file_as_dict['words'])):\n",
    "#     if len(file_as_dict['words'][i]) != len(file_as_dict['bboxes'][i]):\n",
    "#         pprint(file_as_dict['words'][i])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in file_as_dict.keys():\n",
    "#     print(len(file_as_dict[key]), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_output = final_file_line_by_line(file_as_dict, threshold=0.7)\n",
    "df = pd.DataFrame(preprocessed_output)\n",
    "preprocessed_output = df.sort_values(by=['page_id', 'ymin_min']).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_output['full_line'][1], preprocessed_output['bboxes'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the numner of words in each row matches the number of bboxes\n",
    "(df[\"full_line\"].str.len() == df['bboxes'].str.len()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessed_output['page_id']), len(preprocessed_output['full_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output from Rohith that we need to match against\n",
    "\n",
    "section_dict = {1: ('1. Definitions', {}),\n",
    " 2: ('2. Term', {}),\n",
    " 3: ('3. Grant of License and Exclusivity', {}),\n",
    " 4: ('4. Retention of Rights', {}),\n",
    " 5: ('5. Appearances', {}),\n",
    " 6: ('6. Compensation', {}),\n",
    " 7: ('7. Supply of Endorsed Products', {}),\n",
    " 8: ('8. Approval of Advertising', {}),\n",
    " 9: ('9. Ownership', {}),\n",
    " 10: ('10. SAG and/or AFTRA', {}),\n",
    " 11: ('11. Standards', {}),\n",
    " 12: ('12. Events of Default', {}),\n",
    " 13: ('13. Termination/Remedies', {}),\n",
    " 14: ('14. Companys Debts', {}),\n",
    " 15: ('15. Indemnification', {}),\n",
    " 16: ('16. Insurance', {}),\n",
    " 17: ('17. Waiver', {}),\n",
    " 18: ('18. Notices', {}),\n",
    " 19: ('19. Assignment', {}),\n",
    " 20: ('20. Independent Contractor', {}),\n",
    " 21: ('21. Joint Venture', {}),\n",
    " 22: ('22. Governing Law', {}),\n",
    " 23: ('23 Entire Agreement', {}),\n",
    " 24: ('24. Amendments', {}),\n",
    " 25: ('25. Authority', {}),\n",
    " 26: ('26. Severability', {}),\n",
    " 27: ('27. Compliance with Laws', {}),\n",
    " 28: ('28. Attorneys Fees and Costs', {}),\n",
    " 29: ('29. Force Majeure', {}),\n",
    " 30: ('30. Confidentiality', {}),\n",
    " 31: ('31. Counterparts', {})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from fuzzysearch import find_near_matches\n",
    "import re\n",
    "\n",
    "subset_match_threshold = 80    # Design decision to only allow subset match ratios of > 80/100\n",
    "line_len_match_threshold = 0.8 # Design decision to potentially only match document lines\n",
    "                               # that are not much smaller than table of content label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_output'] = df['full_line'].str.join(\" \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(row, toc_page_id, section_dict, line_len_match_threshold, subset_match_threshold):\n",
    "    line, page_id = row['preprocessed_output'], int(row['page_id'])\n",
    "    if page_id != toc_page_id:\n",
    "        for key, section_info in section_dict.items():\n",
    "            if (fuzz.partial_ratio(section_info[0].lower(), line.lower()) > subset_match_threshold and\n",
    "                len(line) >= len(section_info[0]) * line_len_match_threshold):\n",
    "                return line, section_info[0]\n",
    "        else:\n",
    "            return None, None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bboxes(words, query, bboxes, window_size, match_score):\n",
    "    \n",
    "    max_window_score, match_idx = 0, 0\n",
    "    for idx, start_idx in enumerate(range(0, len(words), window_size)):\n",
    "        window_text = \" \".join(words[start_idx : start_idx + window_size])\n",
    "        \n",
    "        window_score = process.extractBests(window_text, [query], scorer=fuzz.token_set_ratio)[0][-1]\n",
    "        candidate_bboxes = bboxes[start_idx : start_idx + window_size]\n",
    "\n",
    "        if window_score >= match_score:\n",
    "            # x min is the x_left of the first bbox \n",
    "            # y_min is the min of the top left y's for each box\n",
    "            x_min = candidate_bboxes[0][0][0]\n",
    "            y_min = min([y[0][-1] for y in candidate_bboxes])\n",
    "\n",
    "            # x max is the x_right of the last bbox \n",
    "            # y_max is the max of the bottom_right y's for each box\n",
    "            x_max = candidate_bboxes[-1][1][0]\n",
    "            y_max = max([y[1][-1] for y in candidate_bboxes])\n",
    "\n",
    "            merged_bbox = [ [x_min, y_min], [x_max, y_max] ]\n",
    "            \n",
    "            return merged_bbox, window_text\n",
    "        \n",
    "        else:\n",
    "            if window_score > max_window_score:\n",
    "                max_window_score = window_score\n",
    "                match_text = window_text\n",
    "                match_candidate_bboxes = candidate_bboxes\n",
    "\n",
    "            continue\n",
    "    \n",
    "    # in case the window_text has a lower match score than with the entire string, match with the \n",
    "    # segment with highest matching score\n",
    "    x_min = match_candidate_bboxes[0][0][0]\n",
    "    y_min = min([y[0][-1] for y in match_candidate_bboxes])\n",
    "\n",
    "    # x max is the x_right of the last bbox \n",
    "    # y_max is the max of the bottom_right y's for each box\n",
    "    x_max = match_candidate_bboxes[-1][1][0]\n",
    "    y_max = max([y[1][-1] for y in match_candidate_bboxes])\n",
    "\n",
    "    merged_bbox = [ [x_min, y_min], [x_max, y_max] ]\n",
    "\n",
    "    return merged_bbox, match_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exact_match(row):\n",
    "    \n",
    "    query = row['match_toc']\n",
    "    if query is not None:\n",
    "        # regex\n",
    "        # row.loc[['match_ocr']].str.extract(rf\"({query})\", flags=re.IGNORECASE, expand=True).values.flatten()[0]\n",
    "        match = process.extractBests(row.loc['match_ocr'], [query], scorer=fuzz.token_set_ratio)\n",
    "        \n",
    "        match_text, match_score = match[0]\n",
    "        window_size = len(match_text.split())\n",
    "\n",
    "        # find the start idx of the first token from the query in the matched string\n",
    "        # query_start_token = query.split()[0]\n",
    "        \n",
    "        \n",
    "        merged_bbox, exact_match_text = merge_bboxes(row['full_line'], query, row['bboxes'], window_size, match_score)\n",
    "        \n",
    "        return merged_bbox, exact_match_text\n",
    "    \n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the page number of the TOC. Skip this during fuzzy matching\n",
    "toc_page_id = df.loc[df['preprocessed_output'].str.contains(r'table of contents', case=False), 'page_id'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['match_ocr'], df['match_toc'] = zip(*df.apply(lambda row: fuzzy_match(row, toc_page_id, section_dict, line_len_match_threshold, subset_match_threshold), axis=1))\n",
    "df.loc[df['match_ocr'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exact_match_bbox'], df['exact_match_text'] = zip(*df.apply(lambda row: extract_exact_match(row), axis=1))\n",
    "df.loc[df['match_ocr'].notnull()].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just the first match for each toc\n",
    "df = df.drop_duplicates(subset=['match_toc'])\n",
    "df.loc[df['match_ocr'].notnull()].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "-  to ensure there's only one match per entry in the TOC (From HTML), we currently keep the first match and drop the rest\n",
    "   -  compare performance of this with condition where we keep the match with the highest score\n",
    "-  extend to more contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matches = []\n",
    "for line in preprocessed_output['full_line']:\n",
    "    line = \" \".join(line) ## DESIGN DECISION TO SPLIT EVERYTHING BY A SPACE\n",
    "    for key, section_info in section_dict.items():\n",
    "        if (fuzz.partial_ratio(section_info[0].lower(), line.lower()) > subset_match_threshold and\n",
    "           len(line) >= len(section_info[0]) * line_len_match_threshold):\n",
    "            matches.append((line, section_info[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see, the strings do somewhat match but weneed to add extra filters in order to remove or ignore matches that we don't want. There might be some combination of string subset matches, full matches, regex, line length, lev distance, etc that we can use to really get good section titles for the TOC output that Rohith gave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have better matches, we can tag each line with the coordinates since that info is stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce6c9754676e356b60dc94fe2b04dd2fb66b8461b319750f347e70624e33de59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
